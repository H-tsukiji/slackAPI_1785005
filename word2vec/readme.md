# 誰でもわかる(？)pythonでword2vecを動かしてみよう

## 環境構築編
word2vecでの単語をベクトル化と類似した単語を算出するには3つのパッケージをインストール  
・gensim  
・numpy(mklが含まれているもの)
・scipy

### インストール方法  
Windowsではwheelでパッケージされたものをダウンロードして，pipで展開＋インストールしよう  

wheelでパッケージ化されたものがあるURL  
http://www.lfd.uci.edu/~gohlke/pythonlibs/

具体例  
numpy‑1.13.0+mkl‑cp36‑cp36m‑win_amd64.whl  
cpはpythonのバージョンを表す。  
cp36の場合，python3.6です。  
環境が32bitか64bitを確認してからダウンロード

コマンドプロンプトでインストール  
pip install numpy‑1.13.0+mkl‑cp36‑cp36m‑win_amd64.whl

うまくいくことを祈る  

## ベースのプログラムを動かす
動くものを使って基本的な処理ができるようになるのを狙う  

### word2vec_train.py
これは学習させて学習モデルを作成するプログラム．  
インプットは分かち書きされた(重要)テキストファイルで行う事．  
学習させる際の関数word2vec.Word2Vec()の引数の解説は以下の通り  
* sg　　　　　　　わからん  
* size  　　　　　ベクトルの次元数  
* min_count　　　n回未満登場する単語を破棄  
* window　　　　　文脈の最大単語数  
* hs　　　　　　　学習に階層化ソフトマトリックスを使用するかどうか  
* negative　　　　ネガティブサンプリングに用いる単語数

実行のコマンド例
>python word2vec_train.py [分かち書きされたテキストファイル].txt

結果  
成功した場合，学習モデルが生成される．具体例の場合は以下のmodelファイルが生成される．  
[分かち書きされたテキストファイル].modelが生成される  

### word2vec_similarity.py  
学習モデルを基に指定された単語に対してベクトルの距離が近い単語上位10個を表示するプログラム  
結果は似ている単語とどれだけ似ているかのスコアが表示される．  
↓の6行目のように，学習したモデルのパスを置いておくこと．  

    model   = word2vec.Word2Vec.load('word2vec-gensim-model/word2vec.gensim.model')

実行コマンド例：python word2vec_similarity.py [指定の単語]  
実行することで，指定された単語と位置買い物が10個表示される．  
またその単語のベクトルを表示する．

出力例  Ubuntuをやった時  

***
SDK      0.942481517791748  
GNOME    0.9394338130950928  
Debian   0.9382969737052917  
Qt       0.9350671768188477  
Microsoft        0.9314861297607422  
Server   0.9276431798934937  
Fedora   0.9276036024093628  
Mozilla          0.9268442988395691  
Apache   0.9222292900085449  
Solaris          0.9128473997116089  


[-0.11026989 -0.1682304  -0.18028475 -0.09590203  0.14725123  0.2540386
  0.01696127 -0.15692762  0.04263042 -0.10862964  0.07641051  0.10044158
  0.02597311  0.11138104  0.0026971  -0.02367262  0.0076061  -0.04597848
 -0.12206826  0.13177207 -0.14126097 -0.07000764 -0.15613353 -0.30948117
  0.28589332  0.07092807  0.09070479  0.26879326 -0.03613377 -0.35026842
 -0.0664842  -0.06576671 -0.06865555 -0.13182332  0.10844455 -0.24455234
 -0.04102402  0.10549691  0.24770316  0.02510064  0.09860197  0.11860157
  0.11387576  0.14463113 -0.11087056 -0.00729228 -0.0055138   0.03761257
 -0.0514333   0.20375614]

***
以下の関数パラメータを変更を変更することで実行結果が変化する．  
* topn:表示させる上位スコアの数  
* positive:対象とする単語(複数でも可能)  
複数にした場合その二つの要素に近いものが表れる．「王」と「日本」とした場合，「王」「日本」の両方の要素を持つとされる単語が抽出される． 
* negative:対象の単語から抜き出す要素  
positiveで出された単語からnegativeのベクトルを引く．[人間]から[男]を引く場合はpositive=”人間”,negative="男"となる．  

``` 
results = model.most_similar(positive=sys.argv[1], topn=10)
```